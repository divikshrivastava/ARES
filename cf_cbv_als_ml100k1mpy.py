# -*- coding: utf-8 -*-
"""CF_CBV_ALS_ML100k1m.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iaL0pIKsJmaRG8RvD3FZ89YeeXiP56NX

# Preprocessing And Compressed Bit Vectors Evaluation
"""

import os
import numpy as np
import pandas as pd
from scipy.sparse import *
from scipy import *
from sklearn.preprocessing import LabelEncoder

## Load dataset Movielens 100K or 1 million rating or bookrating(1million)

# Dataset source:Movielens 100k: https://grouplens.org/datasets/movielens/100k/
# Movielens 1 million: https://grouplens.org/datasets/movielens/1m/
# Book crossing: https://grouplens.org/datasets/book-crossing/

header_list = ["userid", "movieid", "movieRating", "timestamp"]
# data = pd.read_csv('movie100kratings.dat', sep='::', names=header_list)
data = pd.read_csv('movie100kratings.data', sep='\t')
nRow, nCol = data.shape
print(f'There are {nRow} rows and {nCol} columns')
data.head()

data.dtypes

## final dataset sample
sample_data = data.drop(['timestamp'],axis=1)
sample_data.head(10)
#contain 1,000,209 anonymous ratings of approximately 3,900 movies made by 6,040 MovieLens

frequency = sample_data.groupby('userid').count()
print(len(frequency))

test_users = []
for i in range(len(frequency)):
    fre = frequency.iloc[i]
    break
    if (fre[0] > 100):
        test_users.append(i+1)
# print(len(test_users))

test_userid = []
test_movieid = []
test_movieRating = []

print(sample_data.userid.max())

"""create testing set"""

# if the user has rated more than "50" movies then add him to test dataset
for user in range(1,6040): #max userid value from prev step
    filter_df = sample_data.query("userid == " + str(user))
    if (len(filter_df) < 50):
        continue
    # add 20% of that particular user data to the testdata set
    portion = int(len(filter_df)*(0.2))
    random_indices = set(np.random.choice(len(filter_df), portion))
    for i in random_indices:
        row = filter_df.iloc[i]
        test_userid.append(row.userid)
        test_movieid.append(row.movieid)
        test_movieRating.append(row.movieRating)

print(len(test_userid))
print(len(test_movieid))
print(len(test_movieRating))

test_df_dict = {"userid": test_userid, "movieid":test_movieid, "movieRating":test_movieRating}
convert_dict = {
    "userid" :      int,
    "movieid" :     int,
    "movieRating"  :   int
    }
test_df = pd.DataFrame(test_df_dict)
test_df = test_df.astype(convert_dict)
print(test_df.head())
test_df.shape

test_df.to_csv("test_ratings.csv",index=False)
# test dataset csv

# data redundancy
# adding unique userid,movieid tuple to the test_tuple_set
#  trying to avoid users who have rated same movie twice
test_tuple_set = set()
print(len(test_userid))
for i in range(len(test_userid)):
    userid_movieid_tuple = (test_userid[i], test_movieid[i])
    if userid_movieid_tuple in test_tuple_set:
        print(userid_movieid_tuple)
        print(":)")
    test_tuple_set.add(userid_movieid_tuple)
print(len(test_tuple_set))
print(len(test_df))

"""create Training set"""

def drop_rating(row):
    print(len(row))
    t = (row.userid, row.movieid)
    if t in test_tuple_set:
        row.movieRating = 0.0
    return row

# removing test data values/ratings from the training dataset and making them zero not deleting
for i in range(len(sample_data)):
    row = sample_data.iloc[i]
    t = (row.userid, row.movieid)
    if t in test_tuple_set:
        sample_data.at[i,'movieRating'] = 0.0

sample_data.head(10)

sample_data.to_csv("train_ratings.csv", index=False)

len(sample_data.movieid.unique())
# total unique movies/items
# sample_data = whole data

# converting unique movies numpy ndarray to list
unique_movies = sample_data.movieid.unique()
type (list(unique_movies))
print(unique_movies)

""" ***ASSIGN* CONTINUOUS INDEX TO movieid**"""

# adding index to movie id thru a dict
original_movie_ids = []
mapped_movie_ids = []
movie_dict = {}
count = 1
for movie_id in unique_movies:
    original_movie_ids.append(movie_id)
    mapped_movie_ids.append(count)
    movie_dict[movie_id] = count
    count += 1

print(len(original_movie_ids))
print(len(mapped_movie_ids))

# creating a dataframe with colomn1 as uniquemovieids and column2 as mappedmovieids(aka index) , dataframe index vs movie index
movies_map_df_dict = {"original_movie_ids": original_movie_ids,
                      "mapped_movie_ids":mapped_movie_ids}
movies_map_df = pd.DataFrame(movies_map_df_dict)
print(movies_map_df.head())
movies_map_df.to_csv("mapped_movie_ids.csv",index=False)

# adding a column(mapped_movie_id) to orginal wholedataset dataframe sample_data
# len(sample_data)
for i in range(len(sample_data)):
  mapped_movie_id = movie_dict[int(sample_data.iloc[i][1])]
#     print(sample_data.iloc[i][1] ,mapped_movie_id )
  sample_data.loc[sample_data.index[i], 'mapped_movie_id'] = int(mapped_movie_id)

sample_data.head()

users = np.array(sample_data['userid'])
items = np.array(sample_data['movieid'])
ratings = np.array(sample_data['movieRating'])
mapped_movie_ids = np.array(sample_data['mapped_movie_id'])

print(users[3],items[3], ratings[3])

sample_data.movieid.max()

len(users), len(items), len(ratings), len(mapped_movie_ids)

len(sample_data.userid.unique())
# unique users in wholedataset

len(sample_data.movieid.unique())
# unique movies in wholedataset

# sample_data.to_sparse(fill_value=0)

"""Create Utility Matrix"""

# converting original dataframe sample_data to a matrix representation/utility matrix with the help of csr_matrix func of scipy
utility_csr = csr_matrix((ratings, (users , mapped_movie_ids.astype(int))))

utility_matrix = csr_matrix((ratings, (users, mapped_movie_ids.astype(int)))).toarray() # Users x Items
utility_matrix_t = utility_matrix.T
utility_matrix

print(utility_matrix[users[4]][items[4]])

# we have an extra expendable row(likely first row), column in both user_utility_matrix and item_utility_matrix
# shud be 943x1682 / 1682x943 , somewhere csr_matrix func is adding an extra row
print(utility_matrix.shape)
print(utility_matrix_t.shape)

np.savetxt('users_m.txt', utility_matrix, fmt="%d") # All rows as a USERS
np.savetxt('items_m.txt', utility_matrix_t, fmt="%d") # All rows as a ITEMS

test_data = pd.read_csv('test_ratings.csv')
test_users = test_data.userid.unique()
print(len(test_users))
test_users_list = test_users.tolist()
with open ('test_users.txt', 'w') as fo:
     fo.write(','.join(str(i) for i in test_users_list))
# unique test users in testdataset(test_ratings.csv) which is used in javacode for generating recommendations of nearestKneighbors efficiently with compressed bit vectors



## passing(copying) the above 3 txt files
# user_m txt file matrix and
# item_m file matrix, and
# test_users.txt (unique test users in testdataset)
## into java code(intelliJ) for generating recommendation through compressed bit vectors

# below four cells revist

mapped_movie_ids_dict = {}
for key in movie_dict:
    value = movie_dict[key]
    mapped_movie_ids_dict[value] = key

def get_original_movie_ids(mapped_movie_ids):
    original_ids = []
    for movie_id in mapped_movie_ids:
        if int(movie_id) in mapped_movie_ids_dict:
            original_ids.append(mapped_movie_ids_dict[int(movie_id)])
        else:
            original_ids.append(-1)
    return original_ids

temp_str = "6,15,18,19,24,25,26,27,31,32,33,231,291,321,462,470,471,473,477,620"

bsi_mapped_movie_ids = temp_str.split(",")
bsi_original_movie_ids = get_original_movie_ids(bsi_mapped_movie_ids)
print(bsi_original_movie_ids)

# now we evaluate compressed bit vector recommendations ( java code txt files)

import os
import numpy as np
import pandas as pd
from scipy.sparse import *
from scipy import *
from sklearn.preprocessing import LabelEncoder

test_data = pd.read_csv('test_ratings.csv')
nRow, nCol = test_data.shape
print(f'There are {nRow} rows and {nCol} columns')

mapped_movieID_df = pd.read_csv('mapped_movie_ids.csv')

test_data

mapped_movieID_df.head()

# create dictionaries for movieID mapping
mapped_movieID_dict = {}
# mapped_movieID_df.size
for i in range(len(mapped_movieID_df)):
    row = mapped_movieID_df.loc[i]
#     print(row)
    mapped_movieID_dict[row.original_movie_ids] = row.mapped_movie_ids

original_movieID_dict = {}
for i in range(len(mapped_movieID_df)):
    row = mapped_movieID_df.loc[i]
#     print(row)
    original_movieID_dict[row.mapped_movie_ids] = row.original_movie_ids

# unique users in test data for evaluation
test_users = test_data.userid.unique()
test_users_list = test_users.tolist()
print(test_users_list)
print(len(test_users_list))
# used in the java file to generate test_recommendation_bsi.txt and bsi_recommendations_for_all_user.txt

def get_mapped_movieids(original_movieIds):
    mapped_movieIds= []
    for _id in original_movieIds:
        mapped_movieIds.append(mapped_movieID_dict[_id])
    return mapped_movieIds

# get Get movies which is rated (>3.0)/(change depending on dataset) by user in test data for BSI
def get_test_rec_movieIds(userId):
    user_df = test_data[test_data.userid == userId]
    user_movie_df = user_df[user_df.movieRating > 3.0]
    original_movieIds = user_movie_df.movieid.unique()
    mapped_movieIds = get_mapped_movieids(original_movieIds)
    return mapped_movieIds

def find_common(original_ids, rec_ids):
    original_ids_set = set(original_ids)
    common_count = 0
    if rec_ids[-1] == '':
        rec_ids.pop()
    for _id in rec_ids:
        if int(_id) in original_ids_set:
            common_count += 1
    return common_count

mapped_movieID_df.head()

def getOrginalMoviesIds(mapped_movie_ids):
    original_movieIds= []
    for _id in mapped_movie_ids:
        original_movieIds.append(original_movieID_dict[int(_id)])
    return original_movieIds

print(getOrginalMoviesIds(['3','4']))

print(test_data)

# !pip install pyspark

# from pyspark.sql import Row

# def get_watched_movie_map(user_list, test_data):
#     movie_map = {}
#     for user in user_list:
#         watched_movie = test_data.filter(test_data.userid == user).select("movieid").rdd.flatMap(lambda x: x).collect()
#         movie_map[user] = set(watched_movie)
#     return movie_map


# # Sample user IDs
# user_list = [1001, 1002, 1003, 1004]

# # Sample test data DataFrame (using PySpark)
# from pyspark.sql import SparkSession

# spark = SparkSession.builder \
#     .appName("example") \
#     .getOrCreate()

# data = [(1001, 101), (1001, 102), (1001, 103), (1003, 104), (1004, 105), (1004, 106)]
# columns = ["userid", "movieid"]

# test_data = spark.createDataFrame(data, columns)

# # Call the function with the sample values
# movie_map = get_watched_movie_map(user_list, test_data)

# print(movie_map)

# from typing import List
# from collections import defaultdict
# from itertools import islice
# from pyspark.sql import Row

# def get_watched_movie_map(user_list, test_data):
#     movie_map = {}
#     for user in user_list:
#         watched_movie = test_data.filter(test_data.userid == user).select("movieid").rdd.flatMap(lambda x: x).collect()
#         movie_map[user] = set(watched_movie)
#     return movie_map

# def getUpdatedprecison(user_recommendations_map, test_data_map, n):
#     """Calculates precision for each user in a recommendation system.
#     Args:
#         user_recommendations_map: A dictionary mapping user IDs to dictionaries of movie IDs and their predicted ratings.
#         test_data_map: A dictionary mapping user IDs to sets of actual movie IDs they interacted with.
#         n: The number of top recommendations to consider for precision calculation.

#     Returns:
#         A list of precision scores, one for each user.
#     """
#     precision_list = []
#     top_movies_for_users = defaultdict(list)

#     for user, movies_ratings in user_recommendations_map.items():
#         # Sort movies by rating in descending order and limit to top n
#         sorted_movies = sorted(movies_ratings.items(), key=lambda x: x[1], reverse=True)[:n]
#         # Collect top n movie IDs
#         top_n_movies = [movie_id for movie_id, _ in sorted_movies]
#         top_movies_for_users[user] = top_n_movies

#     for user, top_movies in top_movies_for_users.items():
#         common_count = 0
#         if user in test_data_map:
#             # Find the intersection of recommended movies and test data
#             common_count = len(set(top_movies) & set(test_data_map[user]))
#         # Calculate precision and handle division by zero
#         precision = common_count / n if n != 0 else 0
#         precision_list.append(precision)

#     return precision_list

# def getUpdatedRecall(user_recommendations_map, test_data_map, n):
#     """Calculates recall for each user in a recommendation system.

#     Args:
#         user_recommendations_map: A dictionary mapping user IDs to dictionaries of movie IDs and their predicted ratings.
#         test_data_map: A dictionary mapping user IDs to sets of actual movie IDs they interacted with.
#         n: The number of top recommendations to consider for recall calculation.

#     Returns:
#         A list of recall scores, one for each user.
#     """
#     recall_list = []
#     top_movies_for_users = defaultdict(list)

#     # Extract top n recommended movies for each user
#     for user, movies_ratings in user_recommendations_map.items():
#         sorted_movies = sorted(
#             ((movie_id, rating) for movie_id, rating in movies_ratings.items() if rating > 0),
#             key=lambda x: x[1],
#             reverse=True
#         )[:n]
#         top_n_movies = [movie_id for movie_id, _ in sorted_movies]
#         top_movies_for_users[user] = top_n_movies

#     # Calculate recall for each user
#     for user, top_movies in top_movies_for_users.items():
#         common_count = 0
#         if user in test_data_map:
#             # Find the intersection of recommended movies and test data
#             common_count = len(set(top_movies) & test_data_map[user])
#         # Calculate recall and handle division by zero
#         recall = common_count / len(test_data_map[user]) if len(test_data_map[user]) != 0 else 0
#         recall_list.append(recall)

#     return recall_list

# def getUpdatedF1score(precision, recall):
#     f1_list = []
#     for i in range(max(len(precision), len(recall))):
#         f1 = 0
#         if precision[i] + recall[i] > 0:
#             f1 = 2 * precision[i] * recall[i] / (precision[i] + recall[i])
#         f1_list.append(f1)
#     return f1_list

# def calculateAverage(values):
#     if values:
#         return sum(values) / len(values)
#     return 0.0

# normalized = userRecommendationsMap.copy()  # Create a shallow copy
# testDataMap = get_watched_movie_map(userIds, testingData);

# for outer_key, inner_map in normalized.items():
#     for inner_key, value in inner_map.items():
#         if value < 3.8:
#             normalized[outer_key][inner_key] = 0.0  # Set value to 0.0 if below 3.8

# precisionList = getUpdatedprecison(normalized, testDataMap, 5);
# print("For n=5 Precison is: "+ calculateAverage(precisionList))
# precisionList1 = getUpdatedprecison(normalized, testDataMap, 10);
# print("For n=10 Precison is: "+ calculateAverage(precisionList1))
# precisionList2 = getUpdatedprecison(normalized, testDataMap, 25);
# print("For n=25 Precison is: "+ calculateAverage(precisionList2))
# precisionList3 = getUpdatedprecison(normalized, testDataMap, 50);
# print("For n=50 Precison is: "+ calculateAverage(precisionList3))
# precisionList4 = getUpdatedprecison(normalized, testDataMap, 100);
# print("For n=100 Precison is: "+ calculateAverage(precisionList4))

### Calculate Precision and Recall

def getPrecision(total_count, common_count):
    if(total_count ==0):
        return 0
    return (common_count/total_count)

def getRecall(total_count, common_count):
    if(total_count ==0):
        return 1
    return (common_count/total_count)

# def hitratio(total_count, hits):
#     if(hits ==0):
#         return 1
#     return (hits/total_count)

def getf1score(precision, recall):
    if(precision == 0 and recall == 0 ):
        return 0
    return ((2 * precision * recall )/ ( precision + recall ))

### Evalute Bsi algorithm for all test users
## After executing the java code, copy the output
# copy test_recommendation_bsi.txt and bsi_recommendations_for_all_users.txt from the java IntelliJ env into colab jupyterNotebook env

file = open('test_recommendation_bsi.txt', 'a')

# get_test_rec_movieIds
count = 0
input_file = open('test_recommendation_bsi.txt','r') # file pulled from java code. CBV

userIds = []
actual_movies_watched = []
recomendations = []
common_movies = []
similar_usres_k = []
precision = []
recall = []
f1score = []

while (True):
    line = input_file.readline()
    if not line:
        break
    count +=1

    values = line.split(", ")
    userid = values[0]
    userK = values[1] # neighborhood size
    no_recomendations = values[2] # no of recommendations per user
    recomendations_for_user = values[3:] # actual recommendations

    recomendations_for_user.pop() ## removing last null(\n) value

    actual_movies_wated_by_users = get_test_rec_movieIds(int(userid)) # movies in test data  # actaul movies watched by user which are greater > 3
    common_count = find_common(actual_movies_wated_by_users, recomendations_for_user) # movies in algo's predictions

#     print(userId,userK,no_recomendations)
#     print(common_count)

    userIds.append(userid)
    similar_usres_k.append(int(userK))
    actual_movies_watched.append(len(actual_movies_wated_by_users))
    recomendations.append(int(no_recomendations))
    common_movies.append(common_count)

    total_count_for_precision = int(no_recomendations) # actual results
    total_count_for_recall = len(actual_movies_wated_by_users)

    temp_Precision = getPrecision(total_count_for_precision, common_count)
    precision.append(temp_Precision)
    temp_recall = getRecall(total_count_for_recall, common_count)
    recall.append(temp_recall)
    f1score.append(getf1score(temp_Precision, temp_recall))

print(count)
# 568(test_users) * userK(4 len of list) * itemK(5 len of list) = 11360 test_recommendation_bsi.txt rows count

# storing precision and recall in new dict and dataframe
result_df_dict = {"userId":userIds,"actual_movies_watched":actual_movies_watched,
                  "recomendations":recomendations,"common_movies":common_movies,
                  "similar_usres_k":similar_usres_k, "precision": precision, "recall": recall, "f1_score": f1score}

result_bsi_df = pd.DataFrame(result_df_dict)
result_bsi_df.head(25)
result_bsi_df = result_bsi_df.astype({"userId": int})
result_bsi_df

result_bsi_df.sort_values('precision', ascending=False)

result_bsi_df.sort_values('recall', ascending=False)

result_bsi_df.sort_values('f1_score', ascending=False)

result_bsi_df_5 = result_bsi_df.loc[result_bsi_df['recomendations']==5]
result_bsi_df_10 = result_bsi_df.loc[result_bsi_df['recomendations']==10]
result_bsi_df_25 = result_bsi_df.loc[result_bsi_df['recomendations']==25] # final run
result_bsi_df_50 = result_bsi_df.loc[result_bsi_df['recomendations']==50]
result_bsi_df_100 = result_bsi_df.loc[result_bsi_df['recomendations']==100]

def calculateAverage(a):
  pass

result_bsi_df_5.mean()

result_bsi_df_5.max()

result_bsi_df_10.mean()

result_bsi_df_10.max()

result_bsi_df_25.mean()

result_bsi_df_25.max()

result_bsi_df_50.mean()

result_bsi_df_100.mean()

# normalized = userRecommendationsMap.copy()  # Create a shallow copy

# for outer_key, inner_map in normalized.items():
#     for inner_key, value in inner_map.items():
#         if value < 3.8:
#             normalized[outer_key][inner_key] = 0.0  # Set value to 0.0 if below 3.8

# precisionList = getUpdatedprecison(normalized, testDataMap, 5);
# print("For n=5 Precison is: "+ calculateAverage(precisionList))
# precisionList1 = getUpdatedprecison(normalized, testDataMap, 10);
# print("For n=10 Precison is: "+ calculateAverage(precisionList1))
# precisionList2 = getUpdatedprecison(normalized, testDataMap, 25);
# print("For n=25 Precison is: "+ calculateAverage(precisionList2))
# precisionList3 = getUpdatedprecison(normalized, testDataMap, 50);
# print("For n=50 Precison is: "+ calculateAverage(precisionList3))
# precisionList4 = getUpdatedprecison(normalized, testDataMap, 100);
# print("For n=100 Precison is: "+ calculateAverage(precisionList4))

# recallList = getUpdatedrecall(normalized, testDataMap, 5);
# print("For n=5 recall is: "+ calculateAverage(recallList))
# recallList1 = getUpdatedrecall(normalized, testDataMap, 10);
# print("For n=10 recall is: "+ calculateAverage(recallList1))
# recallList2 = getUpdatedrecall(normalized, testDataMap, 25);
# print("For n=25 recall is: "+ calculateAverage(recallList2))
# recallList3 = getUpdatedrecall(normalized, testDataMap, 50);
# print("For n=50 recall is: "+ calculateAverage(recallList3))
# recallList4 = getUpdatedrecall(normalized, testDataMap, 100);
# print("For n=100 recall is: "+ calculateAverage(recallList4))

# f1List = getUpdatedf1(normalized, testDataMap, 5);
# print("For n=5 f1 is: "+ calculateAverage(f1List))
# f1List1 = getUpdatedf1(normalized, testDataMap, 10);
# print("For n=10 f1 is: "+ calculateAverage(f1List1))
# f1List2 = getUpdatedf1(normalized, testDataMap, 25);
# print("For n=25 f1 is: "+ calculateAverage(f1List2))
# f1List3 = getUpdatedf1(normalized, testDataMap, 50);
# print("For n=50 f1 is: "+ calculateAverage(f1List3))
# f1List4 = getUpdatedf1(normalized, testDataMap, 100);
# print("For n=100 f1 is: "+ calculateAverage(f1List4))

###

userId = []
recommendations = []
input_file_1 = open('bsi_recommendations_for_all_users.txt','r')
while (True):
    line = input_file_1.readline()
    if not line:
        break
    count +=1
    values = line.split(", ")
    user_id = values[0]
    recomendations_for_user = values[1:]
    recomendations_for_user.pop()
    actual_movie_ids = getOrginalMoviesIds(recomendations_for_user)
    userId.append(user_id)
    recommendations.append(actual_movie_ids)

print("Finished")

result_all_user_df_dict = {"userID": userId, "recommendations": recommendations}

result_all_user_df = pd.DataFrame(result_all_user_df_dict)

result_all_user_df # one user is missing 685/686 rows are 942 # 3 user missing for 1 mil dataset , no rec for missing user

"""# Collaborative Filtering pyspark ( ALS Baseline)"""

!pip install pyspark

import os
import numpy as np
from pyspark.sql import SparkSession
from pyspark import SparkContext
import pandas as pd
from pyspark.mllib.recommendation import ALS
import math
import pyspark.sql
from pyspark.sql.functions import udf
from pyspark.sql.types import *
from pyspark.ml.evaluation import RegressionEvaluator
import time

# Calling spark session to register application
spark = SparkSession \
    .builder \
    .appName("Recom") \
    .config("spark.recom.demo", "1") \
    .getOrCreate()
# lambda word: (word, 1)

ratings_df = spark.read \
    .format("csv") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .load("train_ratings.csv")

ratings_df.show()
ratings_df.count()

ratings_df = ratings_df.drop('') # reduntant> no values are dropped
ratings_df.count()

"""### Drop test data from training set"""

ratings_df = ratings_df[ratings_df.movieRating != 0.0]
ratings_df.summary
ratings_df.count()

(trainingData,validationData,testData) = ratings_df.randomSplit([0.6,0.2,0.2],5) # randomSplit(weights, seed)

trainingData.show()
print(trainingData.count())
print(validationData.count())
print(testData.count())

validation_for_predict = validationData.select('userid','movieid')
test_for_predict = testData.select('userid','movieid')

test_for_predict.rdd

"""## Using the above data for all the comparable baselines"""

# Common params for ALS
seed = 5
iterations = 10
regularization_parameter = 0.1
ranks = [4, 8, 12]

min_error = 1000
for rank in ranks:
    model = ALS.train(ratings_df, rank, seed=seed, iterations=iterations,
                      lambda_=regularization_parameter)

    #converting prediction into key value pair like key=(userId,movieId) and value = rating
    predictions = model.predictAll(validation_for_predict.rdd).map(lambda r: ((r[0], r[1]), r[2]))

    #joining predicted rating and original ratings to calculate error
    rates_and_preds = validationData.rdd.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)

    error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())

    print ('For rank',rank, "the RMSE is ", error)
    if error < min_error:
        min_error = error
        best_rank = rank

print ("The best model was trained with rank", best_rank)

predictions_test = model.predictAll(test_for_predict.rdd).map(lambda r: ((r[0], r[1]), r[2]))

predictions_test.take(5)

"""### Get Recomendations For User

This method returns original movieId where CBV algorithm returns mapped movieId
"""

def getRecommendations(user,ratings_df,trainDf,model, k): #

    userDf = ratings_df.filter(ratings_df.userid == user) # for eg user = 1 ...getting all movies(rows) user1 has rated

    mov = ratings_df.select('movieid').subtract(userDf.select('movieid')) # mov dataframe wont have whatever user1 has rated

    # measure below two only

    pred_rat = model.predictAll(mov.rdd.map(lambda x: (user, x[0]))).collect()
    print("PR",pred_rat )
    recommendations = sorted(pred_rat, key=lambda x: x[2], reverse=True)[:k]
    print("PR",recommendations)
    return recommendations

model.predict(1,1084) # make sure product id is in the model

user = 1
# how many recommendations you want
k= 10

# Call getRecommendations method
derived_rec = getRecommendations(user, ratings_df, trainingData, model, k)

print ("Movies recommended for:",user)
movie_ids = []
for i in range(len(derived_rec)):
    movie_ids.append(derived_rec[i][1])
#     print (i+1,derived_rec[i][1])

print(movie_ids)
#     movies_df.filter(movies_df.movieId==derived_rec[i][1]).select('title').show()

temp_str = "70, 235, 316, 333, 423, 441, 457, 480, 552, 553, 590, 4006, 72378, 538, 36, 300, 344, 380, 531, 371"

bsi_movie_ids = temp_str.split(", ") # not using bsi_movie_ids
print(bsi_movie_ids)
print(movie_ids)

test_data = pd.read_csv('test_ratings.csv')
test_users = test_data.userid.unique()
print(len(test_users))
test_users_list = test_users.tolist()
userKs = [5] # 10,20,25 final run
itemKs = [5,10,25,50,100] # 5, 10, 25, 50 ,100
len(test_users_list)

# 568(test_users) * userK(4 len of list) * itemK(5 len of list) = 11360 test_recommendation_bsi.txt rows

# def recomendMoviesForAllUsers(test_users, itemKs,testData,trainingData,model):
#     output_file = open("test_recommendations_als_5_10.txt", "a")
#     for u in range(len(test_users)):
#         user = test_users[u]
#         for itemk in itemKs:
#             derived_rec = getRecommendations(user,testData,trainingData,model,itemk)

#             output_file.write(str(user)+", ")
#             output_file.write(str(5)+", ")
#             for i in range(5):
#                 output_file.write(str(derived_rec[i][1])+", ")
#             output_file.write("\n")

#             output_file.write(str(user)+", ")
#             output_file.write(str(itemk)+", ")
#             for i in range(len(derived_rec)):
#                 output_file.write(str(derived_rec[i][1])+", ")
#             output_file.write("\n")
## below snippet final run many itemKs
def recomendMoviesForAllUsers(test_users, itemKs,testData,trainingData,model):
    output_file = open("test_recommendations_als.txt", "a")
    print("total", len(test_users))
    wer = 0
    for u in range(len(test_users)):
      print(wer)
      wer=wer+1
      user = test_users[u]
      for userk in userKs:
        for itemk in itemKs:
            derived_rec = getRecommendations(user,testData,trainingData,model,itemk)
            output_file.write(str(user)+", ")
            # output_file.write(str(userk)+", ")
            output_file.write(str(itemk)+", ")
            for i in range(len(derived_rec)):
                output_file.write(str(derived_rec[i][1])+", ")
            output_file.write("\n")

start = time.time()
derived_rec = recomendMoviesForAllUsers(test_users_list,itemKs,testData,trainingData,model)
end = time.time()
print(end - start)

###  Get movies which is rated (>3.0) by user in test data for ALS

def get_test_rec_movieIds_als(userId):
    user_df = test_data[test_data.userid == userId]
    user_movie_df = user_df[user_df.movieRating > 3.0]
    original_movieIds = user_movie_df.movieid.unique()
    return list(original_movieIds)

# get_test_rec_movieIds
count = 0
input_file = open('test_recommendations_als.txt','r')

userIds = []
actual_movies_watched = []
recomendations = []
common_movies = []
precision = []
recall = []
f1score = []

while (True):
    line = input_file.readline()
    if not line:
        break
    count +=1
    values = line.split(", ")
    userId = values[0]
    no_recomendations = values[1]
    recomendations_for_user = values[2:]

    recomendations_for_user.pop() ## removing last null(\n) value

    actual_movies_watched_by_users = get_test_rec_movieIds_als(int(userId)) # movies in test data
#     print(count)
    common_count = find_common(actual_movies_watched_by_users, recomendations_for_user) # movies in algo's predictions

#     print(userId,userK,no_recomendations)
#     print(common_count)

    userIds.append(userId)
    actual_movies_watched.append(len(actual_movies_watched_by_users))
    recomendations.append(int(no_recomendations))
    common_movies.append(common_count)

    total_count_for_precision = int(no_recomendations)
    total_count_for_recall = len(actual_movies_watched_by_users)

    temp_Precision = getPrecision(total_count_for_precision, common_count)
    precision.append(temp_Precision)
    temp_recall = getRecall(total_count_for_recall, common_count)
    recall.append(temp_recall)
    f1score.append(getf1score(temp_Precision, temp_recall))

print(count)
# test_recommendations_als_5_10.txt row length

result_df_dict = {"userId":userIds,"actual_movies_watched":actual_movies_watched,
                  "recomendations":recomendations,"common_movies":common_movies,
                   "precision": precision, "recall": recall, "f1_score": f1score}

result_als_df = pd.DataFrame(result_df_dict)

result_als_df

result_als_df = result_als_df.astype({"userId": int})

result_als_df_5 = result_als_df.loc[result_als_df['recomendations']==5]
result_als_df_10 = result_als_df.loc[result_als_df['recomendations']==10]
result_als_df_25 = result_als_df.loc[result_als_df['recomendations']==25]
result_als_df_50 = result_als_df.loc[result_als_df['recomendations']==50]
result_als_df_100 = result_als_df.loc[result_als_df['recomendations']==100]

result_als_df_5.mean()

result_als_df_10.mean()

result_als_df_25.mean()

result_als_df_50.mean()

result_als_df_100.mean()

"""## **Collaborative filtering (SVD Baseline)**"""

# !pip install scikit-surprise

# import time
# import pandas as pd
# from surprise import SVD, Dataset, Reader
# from surprise.model_selection import train_test_split
# from surprise import accuracy
# from collections import defaultdict

# # Load the dataset
# ratings_df = pd.read_csv('train_ratings.csv')
# reader = Reader(rating_scale=(1, 5))
# data = Dataset.load_from_df(ratings_df[['userid', 'movieid', 'movieRating']], reader)

# # Split the dataset into training and test sets
# trainset, testset = train_test_split(data, test_size=0.2)

# # Train the SVD model
# svd_model = SVD()
# start_train = time.time()
# svd_model.fit(trainset)
# end_train = time.time()
# print(f"Training time: {end_train - start_train} seconds")

# # Predict ratings for the testset
# start_pred = time.time()
# predictions = svd_model.test(testset)
# end_pred = time.time()
# print(f"Prediction time: {end_pred - start_pred} seconds")

# # Calculate RMSE
# accuracy.rmse(predictions)

# # Function to get top-N recommendations for each user
# def get_top_n(predictions, n=10):
#     top_n = defaultdict(list)
#     for uid, iid, true_r, est, _ in predictions:
#         top_n[uid].append((iid, est))

#     for uid, user_ratings in top_n.items():
#         user_ratings.sort(key=lambda x: x[1], reverse=True)
#         top_n[uid] = user_ratings[:n]

#     return top_n

# top_n = get_top_n(predictions, n=10)

# # Load test data for comparison
# test_data = pd.read_csv('test_ratings.csv')
# test_users = test_data.userid.unique()

# # Function to calculate precision, recall, and F1 score
# def calculate_metrics(top_n, test_data):
#     precision_list = []
#     recall_list = []
#     f1_list = []

#     for user in test_users:
#         # Your actual movie ids here
#         actual_movies = set(test_data[(test_data['userid'] == user) & (test_data['movieRating'] > 3)]['movieid'])
#         # Predicted movie ids
#         predicted_movies = set([iid for (iid, _) in top_n[user]])
#         tp = len(actual_movies & predicted_movies)
#         fp = len(predicted_movies - actual_movies)
#         fn = len(actual_movies - predicted_movies)

#         precision = tp / (tp + fp) if (tp + fp) != 0 else 0
#         recall = tp / (tp + fn) if (tp + fn) != 0 else 0
#         f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0

#         precision_list.append(precision)
#         recall_list.append(recall)
#         f1_list.append(f1_score)

#     return precision_list, recall_list, f1_list

# # Calculate metrics
# precision_list, recall_list, f1_list = calculate_metrics(top_n, test_data)

# # Creating result DataFrame
# result_df_dict = {
#     "userId": list(test_users),
#     "precision": precision_list,
#     "recall": recall_list,
#     "f1_score": f1_list
# }

# result_df = pd.DataFrame(result_df_dict)
# print(result_df)

"""## **Collaborative filtering (Neural Collaborative Filtering Baseline)**


"""

!pip install tensorflow

import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import RootMeanSquaredError
import time

# Load the data (assuming it's already preprocessed)
ratings_df = pd.read_csv("train_ratings.csv")

# Define the neural collaborative filtering model
def create_ncf_model(user_dim, movie_dim):
    user_input = Input(shape=(1,))
    movie_input = Input(shape=(1,))

    user_embedding = Embedding(user_dim+1, 10)(user_input)
    movie_embedding = Embedding(movie_dim+1, 10)(movie_input)

    user_flatten = Flatten()(user_embedding)
    movie_flatten = Flatten()(movie_embedding)

    concat = Concatenate()([user_flatten, movie_flatten])
    dense1 = Dense(64, activation='relu')(concat)
    dense2 = Dense(32, activation='relu')(dense1)
    output = Dense(1)(dense2)

    model = Model(inputs=[user_input, movie_input], outputs=output)
    model.compile(loss='mean_squared_error', optimizer=Adam(), metrics=[RootMeanSquaredError()])

    return model

def getNCFRecommendations(user,ratings_df,model, k): #

    # userDf = ratings_df.filter(ratings_df.userid == user) # for eg user = 1 ...getting all movies(rows) user1 has rated
    # mov = ratings_df.select('movieid').subtract(userDf.select('movieid')) # mov dataframe wont have whatever user1 has rated
    # # measure below two only
    # # predictions = ncf_model.predict([np.array(test_user), np.array(test_movie)]).flatten()
    # pred_rat = model.predict(mov.rdd.map(lambda x: (user, x[0]))).collect()
    # recommendations = sorted(pred_rat, key=lambda x: x[2], reverse=True)[:k]
    # return recommendations


    userDf = ratings_df.filter(ratings_df.userid == user)
    mov = ratings_df.select('movieid').subtract(userDf.select('movieid'))

    # Convert user and movie IDs to numpy arrays
    user_array = np.full(mov.count(), user)
    movie_array = np.array(mov.select('movieid').rdd.flatMap(lambda x: x).collect())

    # Use the NCF model to predict ratings for unrated movies
    predictions = model.predict([user_array, movie_array]).flatten()

    # Combine user, movie, and prediction into a list of tuples
    recommendations = list(zip(user_array, movie_array, predictions))

    # Sort by predicted ratings in descending order and select the top k
    recommendations = sorted(recommendations, key=lambda x: x[2], reverse=True)[:k]

    return recommendations

def recomendNCFMoviesForAllUsers(test_users,itemKs,testData,model):
    output_file = open("test_recommendations_ncf.txt", "a")
    print("total", len(test_users))
    wer = 0
    for u in range(len(test_users)):
      print(wer)
      wer=wer+1
      user = test_users[u]
      for userk in userKs:
        for itemk in itemKs:
            derived_rec = getNCFRecommendations(user,testData,model,itemk)
            output_file.write(str(user)+", ")
            # output_file.write(str(userk)+", ")
            output_file.write(str(itemk)+", ")
            for i in range(len(derived_rec)):
                output_file.write(str(derived_rec[i][1])+", ")
            output_file.write("\n")


user_dim = ratings_df['userid'].nunique()
movie_dim = ratings_df['movieid'].nunique()

# Prepare data for training
train_user = trainingData.select('userid').rdd.flatMap(lambda x: x).collect()
train_movie = trainingData.select('movieid').rdd.flatMap(lambda x: x).collect()
train_rating = trainingData.select('movieRating').rdd.flatMap(lambda x: x).collect()

test_user = testData.select('userid').rdd.flatMap(lambda x: x).collect()
test_movie = testData.select('movieid').rdd.flatMap(lambda x: x).collect()
test_rating = testData.select('movieRating').rdd.flatMap(lambda x: x).collect()


# Create and train the model
ncf_model = create_ncf_model(user_dim, movie_dim)

start_time = time.time()
ncf_model.fit([np.array(train_user), np.array(train_movie)], np.array(train_rating), epochs=10, batch_size=64)
end_time = time.time()
print(f"Time taken for training: {end_time - start_time} seconds")


# # Function to predict all user-movie pairs
# def predict_all(model, user_ids, movie_ids):
#     predictions = model.predict([np.array(user_ids), np.array(movie_ids)]).flatten()
#     return list(zip(user_ids, movie_ids, predictions))

# # Predict all user-movie pairs in the test set
# test_predictions = predict_all(ncf_model, test_user, test_movie)

# Display some test predictions
# print("Sample Test Predictions:")
# for i in range(5):
#     print(test_predictions[i])

# # Time taken for predictions
# start_time = time.time()
# test_predictions = predict_all(ncf_model, test_user, test_movie)
# end_time = time.time()
# print(f"Time taken for predictions: {end_time - start_time} seconds")

# Evaluate the model on the test set

# predictions = ncf_model.predict([np.array(test_user), np.array(test_movie)]).flatten()

# # Create result_df_dict
# result_df_dict_ncf = {
#     "userId": test_user,
#     "actual_movies_watched": len(test_movie),
#     "recomendations": len(test_movie),
#     "common_movies": np.sum(predictions > 3.0),
#     "precision": np.sum(predictions > 3.0) / len(test_movie),
#     "recall": np.sum(predictions > 3.0) / len(test_movie),
#     "f1_score": 2 * (np.sum(predictions > 3.0) / len(test_movie)) / (len(test_movie) + np.sum(predictions > 3.0))
# }

# print(result_df_dict_ncf)

test_data = pd.read_csv('test_ratings.csv')
test_users = test_data.userid.unique()
print(len(test_users))
test_users_list = test_users.tolist()
userKs = [5] # 10,20,25 final run
itemKs = [5,10,25,50,100] # 5, 10, 25, 50 ,100
len(test_users_list)
start = time.time()
derived_rec = recomendNCFMoviesForAllUsers(test_users_list,itemKs,testData,ncf_model)
end = time.time()
print(end - start)

def get_test_rec_movieIds_ncf(userId):
    user_df = test_data[test_data.userid == userId]
    user_movie_df = user_df[user_df.movieRating > 3.0]
    original_movieIds = user_movie_df.movieid.unique()
    return list(original_movieIds)


# get_test_rec_movieIds


count = 0
input_file = open('test_recommendations_ncf.txt','r')

userIds = []
actual_movies_watched = []
recomendations = []
common_movies = []
precision = []
recall = []
f1score = []

while (True):
    line = input_file.readline()
    if not line:
        break
    count +=1
    values = line.split(", ")
    userId = values[0]
    no_recomendations = values[1]
    recomendations_for_user = values[2:]

    recomendations_for_user.pop() ## removing last null(\n) value

    actual_movies_watched_by_users = get_test_rec_movieIds_ncf(int(userId)) # movies in test data
#     print(count)
    common_count = find_common(actual_movies_watched_by_users, recomendations_for_user) # movies in algo's predictions

#     print(userId,userK,no_recomendations)
#     print(common_count)

    userIds.append(userId)
    actual_movies_watched.append(len(actual_movies_watched_by_users))
    recomendations.append(int(no_recomendations))
    common_movies.append(common_count)

    total_count_for_precision = int(no_recomendations)
    total_count_for_recall = len(actual_movies_watched_by_users)

    temp_Precision = getPrecision(total_count_for_precision, common_count)
    precision.append(temp_Precision)
    temp_recall = getRecall(total_count_for_recall, common_count)
    recall.append(temp_recall)
    f1score.append(getf1score(temp_Precision, temp_recall))

print(count)

result_df_dict_ncf = {"userId":userIds,"actual_movies_watched":actual_movies_watched,
                  "recomendations":recomendations,"common_movies":common_movies,
                   "precision": precision, "recall": recall, "f1_score": f1score}

result_ncf_df = pd.DataFrame(result_df_dict_ncf)
result_ncf_df.head(25)
result_ncf_df = result_ncf_df.astype({"userId": int})
result_ncf_df

result_ncf_df_5 = result_ncf_df.loc[result_ncf_df['recomendations']==5]
result_ncf_df_10 = result_ncf_df.loc[result_ncf_df['recomendations']==10]
result_ncf_df_25 = result_ncf_df.loc[result_ncf_df['recomendations']==25] # final run
result_ncf_df_50 = result_ncf_df.loc[result_ncf_df['recomendations']==50]
result_ncf_df_100 = result_ncf_df.loc[result_ncf_df['recomendations']==100]

result_ncf_df_5.mean()

result_ncf_df_10.mean()

result_ncf_df_25.mean()

result_ncf_df_50.mean()

result_ncf_df_100.mean()

# from typing import List
# from collections import defaultdict
# from itertools import islice

# def getUpdatedPrecision(user_recommendations_map, test_data_map, n):
#     """Calculates precision for each user in a recommendation system.
#     Args:
#         user_recommendations_map: A dictionary mapping user IDs to dictionaries of movie IDs and their predicted ratings.
#         test_data_map: A dictionary mapping user IDs to sets of actual movie IDs they interacted with.
#         n: The number of top recommendations to consider for precision calculation.

#     Returns:
#         A list of precision scores, one for each user.
#     """
#     precision_list = []
#     top_movies_for_users = defaultdict(list)

#     for user, movies_ratings in user_recommendations_map.items():
#         # Sort movies by rating in descending order and limit to top n
#         sorted_movies = sorted(movies_ratings.items(), key=lambda x: x[1], reverse=True)[:n]
#         # Collect top n movie IDs
#         top_n_movies = [movie_id for movie_id, _ in sorted_movies]
#         top_movies_for_users[user] = top_n_movies

#     for user, top_movies in top_movies_for_users.items():
#         common_count = 0
#         if user in test_data_map:
#             # Find the intersection of recommended movies and test data
#             common_count = len(set(top_movies) & set(test_data_map[user]))
#         # Calculate precision and handle division by zero
#         precision = common_count / n if n != 0 else 0
#         precision_list.append(precision)

#     return precision_list

# def getUpdatedRecall(user_recommendations_map, test_data_map, n):
#     """Calculates recall for each user in a recommendation system.

#     Args:
#         user_recommendations_map: A dictionary mapping user IDs to dictionaries of movie IDs and their predicted ratings.
#         test_data_map: A dictionary mapping user IDs to sets of actual movie IDs they interacted with.
#         n: The number of top recommendations to consider for recall calculation.

#     Returns:
#         A list of recall scores, one for each user.
#     """
#     recall_list = []
#     top_movies_for_users = defaultdict(list)

#     # Extract top n recommended movies for each user
#     for user, movies_ratings in user_recommendations_map.items():
#         sorted_movies = sorted(
#             ((movie_id, rating) for movie_id, rating in movies_ratings.items() if rating > 0),
#             key=lambda x: x[1],
#             reverse=True
#         )[:n]
#         top_n_movies = [movie_id for movie_id, _ in sorted_movies]
#         top_movies_for_users[user] = top_n_movies

#     # Calculate recall for each user
#     for user, top_movies in top_movies_for_users.items():
#         common_count = 0
#         if user in test_data_map:
#             # Find the intersection of recommended movies and test data
#             common_count = len(set(top_movies) & test_data_map[user])
#         # Calculate recall and handle division by zero
#         recall = common_count / len(test_data_map[user]) if len(test_data_map[user]) != 0 else 0
#         recall_list.append(recall)

#     return recall_list

# normalized = userRecommendationsMap.copy()  # Create a shallow copy

# for outer_key, inner_map in normalized.items():
#     for inner_key, value in inner_map.items():
#         if value < 3.8:
#             normalized[outer_key][inner_key] = 0.0  # Set value to 0.0 if below 3.8

# precisionList = getUpdatedPrecision(normalized, testDataMap, 5);
# print("For n=5 Precison is: "+ calculateAverage(precisionList))
# precisionList1 = getUpdatedPrecision(normalized, testDataMap, 10);
# print("For n=10 Precison is: "+ calculateAverage(precisionList1))
# precisionList2 = getUpdatedPrecision(normalized, testDataMap, 25);
# print("For n=25 Precison is: "+ calculateAverage(precisionList2))
# precisionList3 = getUpdatedPrecision(normalized, testDataMap, 50);
# print("For n=50 Precison is: "+ calculateAverage(precisionList3))
# precisionList4 = getUpdatedPrecison(normalized, testDataMap, 100);
# print("For n=100 Precison is: "+ calculateAverage(precisionList4))

# recallList = getUpdatedrecall(normalized, testDataMap, 5);
# print("For n=5 recall is: "+ calculateAverage(recallList))
# recallList1 = getUpdatedrecall(normalized, testDataMap, 10);
# print("For n=10 recall is: "+ calculateAverage(recallList1))
# recallList2 = getUpdatedrecall(normalized, testDataMap, 25);
# print("For n=25 recall is: "+ calculateAverage(recallList2))
# recallList3 = getUpdatedrecall(normalized, testDataMap, 50);
# print("For n=50 recall is: "+ calculateAverage(recallList3))
# recallList4 = getUpdatedrecall(normalized, testDataMap, 100);
# print("For n=100 recall is: "+ calculateAverage(recallList4))

# f1List = getUpdatedf1(normalized, testDataMap, 5);
# print("For n=5 f1 is: "+ calculateAverage(f1List))
# f1List1 = getUpdatedf1(normalized, testDataMap, 10);
# print("For n=10 f1 is: "+ calculateAverage(f1List1))
# f1List2 = getUpdatedf1(normalized, testDataMap, 25);
# print("For n=25 f1 is: "+ calculateAverage(f1List2))
# f1List3 = getUpdatedf1(normalized, testDataMap, 50);
# print("For n=50 f1 is: "+ calculateAverage(f1List3))
# f1List4 = getUpdatedf1(normalized, testDataMap, 100);
# print("For n=100 f1 is: "+ calculateAverage(f1List4))